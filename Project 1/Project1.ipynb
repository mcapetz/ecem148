{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zNmSdqvNSCZ"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to **M148- Data Science Fundamentals!** This course is designed to equip you with the tools and experiences necessary to start you off on a life-long exploration of datascience. We do not assume a prerequisite knowledge or experience in order to take the course. \n",
    "\n",
    "For this first project we will introduce you to the end-to-end process of doing a datascience project. Our goals for this project are to:\n",
    "\n",
    "1. Familiarize you with the development environment for doing datascience\n",
    "2. Get you comfortable with the python coding required to do datascience\n",
    "3. Provide you with an sample end-to-end project to help you visualize the steps needed to complete a project on your own\n",
    "4. Ask you to recreate a similar project on a separate dataset\n",
    "\n",
    "In this project you will work through an example project end to end. Many of the concepts you will encounter will be unclear to you. That is OK! The course is designed to teach you these concepts in further detail. For now our focus is simply on having you replicate the code successfully and seeing a project through from start to finish. \n",
    "\n",
    "Here are the main steps:\n",
    "\n",
    "1. Get the data\n",
    "2. Visualize the data for insights\n",
    "3. Preprocess the data for your machine learning algorithm\n",
    "4. Select a model and train\n",
    "5. Does it meet the requirements? Fine tune the model\n",
    "\n",
    "![steps](images/MLProcess.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqh5VsNGNSCa"
   },
   "source": [
    "## Working with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gApbdGTXNSCa"
   },
   "source": [
    "It is best to experiment with real-data as opposed to aritifical datasets. \n",
    "\n",
    "There are many different open datasets depending on the type of problems you might be interested in!\n",
    "\n",
    "Here are a few data repositories you could check out:\n",
    "- [UCI Datasets](https://archive.ics.uci.edu/ml/)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/)\n",
    "- [AWS Datasets](https://registry.opendata.aws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIu1SBD4NSCa"
   },
   "source": [
    "## Submission Instructions\n",
    "**Project is due April 26th at 12:00 pm noon. To submit the project, please save the notebook as a pdf file and submit the assignment via Gradescope. In addition,  Make sure that all figures are legible and sufficiently large.**\n",
    "\n",
    "\n",
    "# Example Datascience Exercise\n",
    "Below we will run through an California Housing example collected from the 1990's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V35F05orNSCb"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R8jvlfyZNSCb"
   },
   "source": [
    "Before getting started, it is always good to check the versions of important packages. Knowing the version number makes it easier to lookup correct documenation. \n",
    "\n",
    "To run this project, you will need the following packages installed with at least the minimial version number provided:\n",
    "- Python Version >= 3.9\n",
    "- Scitkit-learn >= 1.0.2\n",
    "- Numpy >= 1.18.5\n",
    "- Scipy >= 1.1.0\n",
    "- Pandas >= 1.4.0\n",
    "- Matplotlib >= 3.3.2\n",
    "\n",
    "The following code imports these packages and checks their version number. If any assertion error occurs, you may not have the correct version installed.\n",
    "\n",
    "**Important: If installed using a package manager like Anaconda or pip, these dependencies should be resolved. Please follow the python setup guide provided during discussion of week 1. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1415,
     "status": "ok",
     "timestamp": 1680224962487,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "Byam08itNSCb"
   },
   "outputs": [],
   "source": [
    "#Import and Version Test\n",
    "#Python version test\n",
    "import sys\n",
    "assert sys.version_info >= (3, 9) # python>=3.9\n",
    "\n",
    "#Machine learning library\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0.2\" # sklearn >= 1.0.2\n",
    "\n",
    "#numerical packages in python\n",
    "import numpy as np \n",
    "assert np.__version__ >= \"1.18.5\" # numpy >= 1.18.5\n",
    "\n",
    "#Another numerical package, unused directly but is implicitly used in sklearn\n",
    "#Check the version just in case\n",
    "import scipy as scp\n",
    "assert scp.__version__ >= \"1.1.0\" # scipy >= 1.1.0\n",
    "\n",
    "#Package for data manipulation and analysis\n",
    "import pandas as pd\n",
    "assert pd.__version__ >= \"1.4.0\" # pandas >= 1.4.0\n",
    "\n",
    "#matplotlib magic for inline figures\n",
    "import matplotlib # plotting library\n",
    "assert matplotlib.__version__ >= \"3.3.2\" # matplotlib >= 3.3.2\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1680224962488,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "AdlwzYX6NSCc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "DATASET_PATH = os.path.join(\"datasets\", \"housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1680224962488,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "ieCSLaJ5NSCc"
   },
   "outputs": [],
   "source": [
    "#Other setup with necessary plotting\n",
    "\n",
    "#Instead of using matplotlib direclty, we will use their nice pyplot interface defined as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed to make this notebook's output identical at every run\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting Utilities\n",
    "\n",
    "# Where to save the figures\n",
    "ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    '''\n",
    "        plt.savefig wrapper. refer to \n",
    "        https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html\n",
    "    '''\n",
    "    path = os.path.join(IMAGES_PATH, fig_name + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_name)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRrxyOTQNSCc"
   },
   "source": [
    "## Step 1. Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGgCVUiLNSCc"
   },
   "source": [
    "### Intro to Data Exploration Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABf5wwstNSCc"
   },
   "source": [
    "In this section we will load the dataset, do some cleaning, and visualize different\n",
    "features using different types of plots.\n",
    "\n",
    "Packages we will use:\n",
    "- **[Pandas](https://pandas.pydata.org):** is a fast, flexibile and expressive data structure widely used for tabular and multidimensional datasets.\n",
    "- **[Matplotlib](https://matplotlib.org)**: is a 2d python plotting library which you can use to create quality figures (you can plot almost anything if you're willing to code it out!)\n",
    "    - other plotting libraries: [seaborn](https://seaborn.pydata.org), [ggplot2](https://ggplot2.tidyverse.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1680224962489,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "Ky7RJulANSCc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAj-DIxvNSCc"
   },
   "source": [
    "First, we load the dataset into pandas Dataframe which you can think about as an array/table. The Dataframe has a lot of useful functionality which we will use throughout the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1680224962489,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "9R4qD90LNSCd",
    "outputId": "d10f887f-8255-4e37-9b25-c13a80fd5b7d"
   },
   "outputs": [],
   "source": [
    "housing = load_housing_data(DATASET_PATH) # we load the pandas dataframe\n",
    "housing.head() # show the first few elements of the dataframe\n",
    "               # typically this is the first thing you do\n",
    "               # to see how the dataframe looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa7w-Qn1NSCd"
   },
   "source": [
    "A dataset may have different types of features\n",
    "- real valued\n",
    "- Discrete (integers)\n",
    "- categorical (strings)\n",
    "\n",
    "The two categorical features are essentialy the same as you can always map a categorical string/character to an\n",
    "integer. \n",
    "\n",
    "In the dataset example, all our features are real valued floats, except ocean proximity which is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1680224962490,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "JDkkfdhtNSCd"
   },
   "outputs": [],
   "source": [
    "# to see a concise summary of data types, null values, and counts\n",
    "# use the info() method on the dataframe\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1680224962490,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "xSBv7ouJNSCd"
   },
   "outputs": [],
   "source": [
    "# you can access individual columns similarly\n",
    "# to accessing elements in a python dict\n",
    "print(housing[\"ocean_proximity\"].head()) # added head() to avoid printing many columns.\n",
    "\n",
    "#Additionally, columns can be accessed as attirbutes of the dataframe object\n",
    "#This method is convenient to access data but should be used with care since you can't overwrite\n",
    "#built in functions like housing.min()\n",
    "print(housing.ocean_proximity.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1680224962490,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "j2wM8U7fNSCd"
   },
   "outputs": [],
   "source": [
    "# to access a particular row we can use iloc\n",
    "housing.iloc[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1680224962491,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "-Qr3mKL4NSCd"
   },
   "outputs": [],
   "source": [
    "# one other function that might be useful is\n",
    "# value_counts(), which counts the number of occurences\n",
    "# for categorical features\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2170,
     "status": "aborted",
     "timestamp": 1680224962491,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "CmT9B1LNNSCd"
   },
   "outputs": [],
   "source": [
    "# The describe function compiles your typical statistics for each non-categorical column\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oINECDG9NSCd"
   },
   "source": [
    "We can also perform groupings based on categorical values and analyze each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2171,
     "status": "aborted",
     "timestamp": 1680224962492,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "l7Yqf4NGNSCd"
   },
   "outputs": [],
   "source": [
    "housing_group = housing.groupby('ocean_proximity')\n",
    "#Has the mean for every column grouped by ocean proximity\n",
    "housing_mean = housing_group.mean()\n",
    "housing_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2170,
     "status": "aborted",
     "timestamp": 1680224962492,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "PTCHEQMONSCe"
   },
   "outputs": [],
   "source": [
    "#We can also get the subset of data associated with that group\n",
    "\n",
    "housing_inland = housing_group.get_group(\"INLAND\")\n",
    "housing_inland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2169,
     "status": "aborted",
     "timestamp": 1680224962492,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "s_u20LVnNSCe"
   },
   "outputs": [],
   "source": [
    "#We can thus performs operations on each group separately\n",
    "housing_inland.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQXjhDjMNSCe"
   },
   "source": [
    "**Grouping is a powerful technique within pandas and a recommend reading the user guide to understand it better [here](https://pandas.pydata.org/docs/user_guide/groupby.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_lPlF9ENSCe"
   },
   "source": [
    "In addition to grouping, we can also filter out the data based on our desired criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962945,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "FcgiJ1wONSCe"
   },
   "outputs": [],
   "source": [
    "housing_expensive= housing[(housing[\"median_house_value\"] > 50000)]\n",
    "housing_expensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962946,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "a6Is_1ZiNSCe"
   },
   "outputs": [],
   "source": [
    "#We can combine multiple criteria \n",
    "housing_expensive_small= housing[(housing[\"median_house_value\"] > 50000)& (housing[\"population\"] < 1000)]\n",
    "housing_expensive_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrBhldD8NSCe"
   },
   "source": [
    "**If you want to learn about different ways of accessing elements or other functions it's useful to check out the getting started section of pandas [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html) and for a full look at all the functionaltiy that pandas offers you can check out the user guide of pandas [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypzIRKlyNSCe"
   },
   "source": [
    "## Step 2. Visualizing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hjhmQ-ANSCe"
   },
   "source": [
    "### Let's start visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962946,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "vuzXNicONSCe"
   },
   "outputs": [],
   "source": [
    "# We can draw a histogram for each of the dataframes features\n",
    "# using the built-in hist function of Dataframe\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "#save_fig(\"attribute_histogram_plots\")\n",
    "plt.show() # pandas internally uses matplotlib, and to display all the figures\n",
    "           # the show() function must be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962946,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "WmPx9KAqNSCe"
   },
   "outputs": [],
   "source": [
    "# if you want to have a histogram on an individual feature:\n",
    "housing[\"median_income\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962946,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "ypUNDVbDNSCf"
   },
   "outputs": [],
   "source": [
    "#You can even plot histograms by specifying the groupings using by \n",
    "housing[\"median_income\"].hist(by= housing[\"ocean_proximity\"],figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962947,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "ZJoS5IKENSCf"
   },
   "outputs": [],
   "source": [
    "#We can also plot statistics of each groupings\n",
    "housing_group_mean = housing.groupby(\"ocean_proximity\").mean()\n",
    "\n",
    "housing_group_mean.plot.bar(y =\"median_income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsDvjLs_NSCf"
   },
   "source": [
    "We can convert a floating point feature to a categorical feature\n",
    "by binning or by defining a set of intervals. \n",
    "\n",
    "For example, to bin the households based on median_income we can use the pd.cut function. Note that we use np.inf to represent infinity which is internally handeled. Thus, the last bin is $(6,\\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962947,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "DArweasENSCf"
   },
   "outputs": [],
   "source": [
    "# assign each bin a categorical value [1, 2, 3, 4, 5] in this case.\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962947,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "tC6d_z7VNSCf"
   },
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl6BMXb9NSCf"
   },
   "source": [
    "**Next let's visualize the household incomes based on latitude & longitude coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962947,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "fLsxoAj5NSCf"
   },
   "outputs": [],
   "source": [
    "## here's a not so interesting way of plotting it\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "#save_fig(\"bad_visualization_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962948,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "GLw-xurENSCf"
   },
   "outputs": [],
   "source": [
    "# we can make it look a bit nicer by using the alpha parameter, \n",
    "# it simply plots less dense areas lighter.\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "#save_fig(\"better_visualization_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962948,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "KR6yAcvpNSCf"
   },
   "outputs": [],
   "source": [
    "# A more interesting plot is to color code (heatmap) the dots\n",
    "# based on income. The code below achieves this\n",
    "\n",
    "# load an image of california\n",
    "images_path = os.path.join('./', \"images\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "filename = \"california.png\"\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "california_img=mpimg.imread(os.path.join(images_path, filename))\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                       s=housing['population']/100, label=\"Population\",\n",
    "                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                       colorbar=False, alpha=0.4,\n",
    "                      )\n",
    "# overlay the califronia map on the plotted scatter plot\n",
    "# note: plt.imshow still refers to the most recent figure\n",
    "# that hasn't been plotted yet.\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "# setting up heatmap colors based on median_house_value feature\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cb.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "#save_fig(\"california_housing_prices_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkhSuAMSNSCg"
   },
   "source": [
    "Not suprisingly, we can see that the most expensive houses are concentrated around the San Francisco/Los Angeles areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zjr27CPZNSCg"
   },
   "source": [
    "Up until now we have only visualized feature histograms and basic statistics. \n",
    "\n",
    "When developing machine learning models the predictiveness of a feature for a particular target of interest is what's important.\n",
    "\n",
    "It may be that only a few features are useful for the target at hand, or features may need to be augmented by applying certain transformations. \n",
    "\n",
    "Nonetheless we can explore this using correlation matrices. Each row and column of the correlation matrix represents a non-categorical feature in our dataset and each element specifies the correlation between the row and column features. [Correlation](https://en.wikipedia.org/wiki/Correlation) is a measure of how the change in one feature affects the other feature. For example, a positive correlation means that as one feature gets larger, then the other feature will also generally get larger. Note that a feature is always fully correlated to itself which is why the diagonal of the correlation matrix is just all 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1680224962948,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "MhkRhlw5NSCg"
   },
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962949,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "7gmafaocNSCg"
   },
   "outputs": [],
   "source": [
    "# for example if the target is \"median_house_value\", most correlated features can be sorted\n",
    "# which happens to be \"median_income\". This also intuitively makes sense.\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962949,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "-dLTbFgBNSCg"
   },
   "outputs": [],
   "source": [
    "# We can plot a scatter matrix for different attributes/features \n",
    "# to see how some features may show a positive correlation/negative correlation or\n",
    "# it may turn out to be completely random!\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "#save_fig(\"scatter_matrix_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962949,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "JxqJGm2gNSCg"
   },
   "outputs": [],
   "source": [
    "# median income vs median house value plot 2 in the first row of top figure\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "#save_fig(\"income_vs_house_value_scatterplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrNcuhXINSCg"
   },
   "source": [
    "### Augmenting Features: Simple Example\n",
    "New features can be created by combining different columns from our data set.\n",
    "\n",
    "- rooms_per_household = total_rooms / households\n",
    "- bedrooms_per_room = total_bedrooms / total_rooms\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962949,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "wwKpaIK6NSCg"
   },
   "outputs": [],
   "source": [
    "#A new column in the dataframe can be made the same away you add a new element to a dict\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962949,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "eG81t_ieNSCg"
   },
   "outputs": [],
   "source": [
    "# obtain new correlations\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1680224962950,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "adUWT68aNSCg"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962950,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "QplWTyG0NSCh"
   },
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MJXtPDLNSCh"
   },
   "source": [
    "### Augmenting Features: Advanced Example\n",
    "In addition to augmenting the data using these simple operations, we can also do some advanced augmentation by bringing information from another dataset. \n",
    "\n",
    "In this case, we are going to find the distance between the houses and the 10 biggest cities in California during 1990. Intuitively, the location of major cities can strongly impact the value of a home. Thus, our new feature will be the distance of the home to the closest big city among the 10 biggest cities.\n",
    "\n",
    "To perform this feature extraction, we will use the provided dataset \"city_data.csv\". We will also employ some helper functions and use the pd.apply function to do the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "aborted",
     "timestamp": 1680224962950,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "XuotgBlhNSCh"
   },
   "outputs": [],
   "source": [
    "#Loads the city data\n",
    "def load_city_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"city_data.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "city_data = load_city_data(DATASET_PATH)\n",
    "city_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1680224962954,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "VrRk4aT6NSCh"
   },
   "outputs": [],
   "source": [
    "#For ease of use, we will convert city_data into a python dict \n",
    "#where the key is the city name and the value is the coordinates\n",
    "city_dict = {}\n",
    "for dat in city_data.iterrows(): #iterates through the rows of the dataframe\n",
    "    row = dat[1]    \n",
    "    city_dict[row[\"City\"]] = (row[\"Latitude\"],row[\"Longitude\"])\n",
    "    \n",
    "print(city_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962955,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "P-G-SILZNSCh"
   },
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#This function is used to calculate the distance between two points on a latitude and longitude grid.\n",
    "#You don't need to understand the math but know that it takes into account the curverature of the earth\n",
    "#to make an accurate distance measurement. \n",
    "#While we could have used the geopy package to do this for us, this way we don't have to install it.\n",
    "def distance_func(loc_a,loc_b):\n",
    "    \"\"\"\n",
    "    Calculates the haversine distance between coordinates\n",
    "    on the latitude and longitude grid. \n",
    "    Distance is in km.\n",
    "    \"\"\"\n",
    "    lat1,lon1 = loc_a\n",
    "    lat2,lon2 = loc_b\n",
    "    r = 6371\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "    return np.round(res, 2)\n",
    "\n",
    "\n",
    "#Calculates closest point to the location given in kilometers\n",
    "def closest_point(location, location_dict):\n",
    "    \"\"\" take a tuple of latitude and longitude and \n",
    "        compare to a dictionary of locations where\n",
    "        key = location name and value = (lat, long)\n",
    "        returns tuple of (closest_location , distance) \n",
    "        distance is in kilometers\"\"\"\n",
    "    closest_location = None\n",
    "    for city in location_dict.keys():\n",
    "        distance = distance_func(location, location_dict[city])\n",
    "        if closest_location is None:\n",
    "            closest_location = (city, distance)\n",
    "        elif distance < closest_location[1]:\n",
    "            closest_location = (city, distance)\n",
    "    return closest_location\n",
    "\n",
    "#Example\n",
    "closest_point((37.774931,-120.419417), city_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962955,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "KPnxRfpaNSCh"
   },
   "outputs": [],
   "source": [
    "#Now we apply the closest_point function to every data point in housing\n",
    "#Axis = 1 specifies that apply will send each row one by one into the designated function\n",
    "#We use the lambda function to catch the row and then disperse its arguments into closest_point\n",
    "housing['close_city'] = housing.apply(lambda x: closest_point((x['latitude'],x['longitude']),city_dict), axis = 1)\n",
    "\n",
    "#Since closest point outputed a tuple of names and distance, we have to split it up. \n",
    "housing['close_city_name'] = [x[0] for x in housing['close_city'].values]\n",
    "housing['close_city_dist'] = [x[1] for x in housing['close_city'].values]\n",
    "\n",
    "#Drop the redundant column\n",
    "housing = housing.drop('close_city', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962955,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "PF2WhkFgNSCh"
   },
   "outputs": [],
   "source": [
    "#Now, let us look at our new features\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962955,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "bv2ukSjzNSCh"
   },
   "outputs": [],
   "source": [
    "#We can also look at the new statistics\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEjokv_3NSCh"
   },
   "source": [
    "Now, let us see if the new feature provides some information about housing prices by looking at the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962956,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "XxUpOM0qNSCh"
   },
   "outputs": [],
   "source": [
    "# obtain new correlations\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962956,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "I49UXHmBNSCi"
   },
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"close_city_dist\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 450, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGZMUsxINSCi"
   },
   "source": [
    "**Observation**: From the correlation, we can see a negative correlation implying that the farther a house is from a big city, the less it costs. From the plot, we can confirm the negative correlation. We can also note that most houses are within 250 km of the big cities which can indicate that everything past 250 is an outlier or should be treated differently like farm land. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYruxJFqNSCi"
   },
   "source": [
    "## Step 3. Preprocess the data for your machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3I85eWbNSCi"
   },
   "source": [
    "Once we've visualized the data, and have a certain understanding of how the data looks like. It's time to clean!\n",
    "\n",
    "Most of your time will be spent on this step, although the datasets used in this project are relatively nice and clean... in the real world it could get real dirty.\n",
    "\n",
    "After having cleaned your dataset you're aiming for:\n",
    "- train set\n",
    "- test set\n",
    "\n",
    "In some cases you might also have a validation set as well for tuning hyperparameters (don't worry if you're not familiar with this term yet..)\n",
    "\n",
    "In supervised learning setting your train set and test set should contain (**feature**, **target**) tuples. \n",
    " - **feature**: is the input to your model\n",
    " - **target**: is the ground truth label\n",
    "     - when target is categorical the task is a classification task\n",
    "     - when target is floating point the task is a regression task\n",
    "     \n",
    "We will make use of **[scikit-learn](https://scikit-learn.org/stable/)** python package for preprocessing. \n",
    "\n",
    "Scikit learn is pretty well documented and if you get confused at any point simply look up the function/object [here](https://scikit-learn.org/stable/user_guide.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWbnp0d4NSCi"
   },
   "source": [
    "### Dealing With Incomplete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962956,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "RYyr6ugLNSCi"
   },
   "outputs": [],
   "source": [
    "# have you noticed when looking at the dataframe summary certain rows \n",
    "# contained null values? we can't just leave them as nulls and expect our\n",
    "# model to handle them for us so we'll have to devise a method for dealing with them...\n",
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962957,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "nXAI0LjtNSCi"
   },
   "outputs": [],
   "source": [
    "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1: simply drop rows that have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962957,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "HzX3Qew0NSCi"
   },
   "outputs": [],
   "source": [
    "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2: drop the complete feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962957,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "Xl5-MXslNSCi"
   },
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median() \n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3: replace na values with median values\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLeapHr_NSCi"
   },
   "source": [
    "The option where we replace the null values with a new number is known as [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)).\n",
    "\n",
    "Could you think of another plausible imputation for this dataset instead of using the median? (Not graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_UsxpDfNSCi"
   },
   "source": [
    "### Using Scikit-learn transformers to preprocess data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRBSXq3DNSCj"
   },
   "source": [
    "We have shown some operations that we want to perform on the dataset. While it is possible to manually perform it all yourselves, it is much easier to offload some of the work to the many fantastic machine learning packages. One such example is scikit-learn where we will demonstrate the use of a transformer to handle some of the work.\n",
    "\n",
    "Consider a situation where we want to normalize the data for each feature. This involves calculating the mean $\\mu$ and standard deviation $\\sigma$ for that feature and applying $\\frac{z-\\mu}{\\sigma}$ where $z$ is the feature value. We will show how to perform this using StandardScalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1680224962957,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "_EbHnX6uNSCj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Extract two real valued columns\n",
    "housing_sub = housing[[\"housing_median_age\",\"total_rooms\"]]\n",
    "\n",
    "scaler = StandardScaler() #initiate class\n",
    "#Calling .fit lets scaler calculate the mean and standard deviation, i.e. trains the standardizer\n",
    "scaler.fit(housing_sub)\n",
    "print(\"Mean: \",scaler.mean_)\n",
    "print(\"Std: \",scaler.scale_)\n",
    "\n",
    "#To perform the standardization, use the .transform function\n",
    "housing_std= scaler.transform(housing_sub)\n",
    "print(\"Transfrom output\")\n",
    "print(housing_std)\n",
    "\n",
    "#As a shorthand, the function .fit_transform performs both operations\n",
    "housing_std_2= scaler.fit_transform(housing_sub)\n",
    "print(\"Fit Transfrom output\")\n",
    "print(housing_std_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33EQ_RR4NSCj"
   },
   "source": [
    "### Prepare Data using a pipeline\n",
    "\n",
    "Now, we will show how we can use scikit learn to create a pipeline that performs all the data preparation in one clean function call. For simplicity, we will not perform the closest city feature extraction in this pipeline. \n",
    "\n",
    "It is very useful to combine several steps into one to make the process much simpler to understand and easy to alter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962958,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "cuS7Jnz3NSCj"
   },
   "outputs": [],
   "source": [
    "housing = load_housing_data(DATASET_PATH) # Load the dataset\n",
    "\n",
    "housing_features = housing.drop(\"median_house_value\", axis=1) # drop labels for training set features\n",
    "                                                       # the input to the model should not contain the true label\n",
    "housing_target = housing[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962958,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "x0X0uABVNSCj"
   },
   "outputs": [],
   "source": [
    "housing_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962959,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "O_XTVwPvNSCj"
   },
   "outputs": [],
   "source": [
    "# This cell implements the complete pipeline for preparing the data\n",
    "# using sklearns TransformerMixins\n",
    "# Earlier we mentioned different types of features: categorical, and floats.\n",
    "# In the case of floats we might want to convert them to categories.\n",
    "# On the other hand categories in which are not already represented as integers must be mapped to integers before\n",
    "# feeding to the model.\n",
    "\n",
    "# Additionally, categorical values could either be represented as one-hot vectors or simple as normalized/unnormalized integers.\n",
    "# Here we encode them using one hot vectors.\n",
    "\n",
    "# DO NOT WORRY IF YOU DO NOT UNDERSTAND ALL THE STEPS OF THIS PIPELINE. CONCEPTS LIKE NORMALIZATION, \n",
    "# ONE-HOT ENCODING ETC. WILL ALL BE COVERED IN DISCUSSION\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    ", →\n",
    "\n",
    "######Processing Real Valued Features\n",
    "# column indices\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class AugmentFeatures(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    implements the previous features we had defined\n",
    "    housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "    housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "    housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "    '''\n",
    "    def __init__(self, add_bedrooms_per_room = True): \n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        #Note that we do not use the pandas indexing anymore\n",
    "        #This is due to sklearn transforming the dataframe into a numpy array during the processing\n",
    "        #Thus, depending on where AugmentFeatures is in the pipeline, a different input type can be expected\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "#Example of using AugmentFeatures\n",
    "housing_features_num = housing_features.drop(\"ocean_proximity\", axis=1) # remove the categorical features\n",
    "attr_adder = AugmentFeatures(add_bedrooms_per_room=False) #Create transformer object\n",
    "housing_extra_attribs = attr_adder.transform(housing_features_num.values) #housing_num.values extracts the numpy array of the datafram\n",
    "\n",
    "print(\"Example of Augment Features Transformer\")\n",
    "print(housing_extra_attribs[0])\n",
    "\n",
    "\n",
    "#Pipiline for real valued features\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), #Imputes using median\n",
    "        ('attribs_adder', AugmentFeatures(add_bedrooms_per_room=True)), #\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "#Example\n",
    "#Output is a numpy array\n",
    "housing_features_num_tr = num_pipeline.fit_transform(housing_features_num)\n",
    "print(\"Example Output of Pipeline for numerical output\")\n",
    "print(housing_features_num_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962959,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "eMhelJL0NSCj"
   },
   "outputs": [],
   "source": [
    "#Full Pipeline\n",
    "\n",
    "#Splits names into numerical and categorical features\n",
    "numerical_features = list(housing_features_num)\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "\n",
    "#Applies different transformations on numerical columns vs categorial columns\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "#Example of full pipeline\n",
    "#Output is a numpy array\n",
    "housing_prepared = full_pipeline.fit_transform(housing_features)\n",
    "print(\"Example Output of full Pipeline\")\n",
    "print(housing_prepared[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtLawTRdNSCj"
   },
   "source": [
    "Now, we have a pipeline that easily processes the input data into our desired form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBNNr44lNSCk"
   },
   "source": [
    "### Splitting our dataset\n",
    "\n",
    "First we need to carve out our dataset into a training and testing cohort. To do this we'll use train_test_split, a very elementary tool that arbitrarily splits the data into training and testing cohorts.\n",
    "\n",
    "Note that we first perform the train test split on the data before it was processed in the pipeline and then separatelyprocess the train and test data. This is done to avoid injecting information into the test data from the train data such filling in missing values in the test data with knowledge of the train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962959,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "iHDImQuJNSCk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_target = housing['median_house_value']\n",
    "train, test, target, target_test = train_test_split(housing_features, data_target, test_size=0.3, random_state=0)\n",
    "\n",
    "train = full_pipeline.fit_transform(train)\n",
    "test = full_pipeline.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HijAeh8ENSCk"
   },
   "source": [
    "### Select a model and train\n",
    "\n",
    "Once we have prepared the dataset it's time to choose a model.\n",
    "\n",
    "As our task is to predict the median_house_value (a floating value), regression is well suited for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962959,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "XvtqoGehNSCk"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Instantiate a linear regresion class\n",
    "lin_reg = LinearRegression()\n",
    "#Train the class using the .fit function\n",
    "lin_reg.fit(train, target)\n",
    "\n",
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "data = test\n",
    "labels = target_test\n",
    "\n",
    "#Uses predict to get the predicted target values\n",
    "print(\"Predictions:\", lin_reg.predict(data)[:5])\n",
    "print(\"Actual labels:\", list(labels)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962959,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "5efc9XyvNSCk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = lin_reg.predict(test)\n",
    "mse = mean_squared_error(target_test, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45UbOD3kNSCk"
   },
   "source": [
    "# TODO: Applying the end-end ML steps to a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjpKMV-DNSCk"
   },
   "source": [
    "We will apply what we've learnt to another dataset ([NYC airbnb dataset from 2019](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)). We will predict airbnb price based on other features. \n",
    "\n",
    "Note: You do not have to use only one cell when programming your code and can do it over multiple cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0TFrbQ0NSCk"
   },
   "source": [
    "## [50 pts] Visualizing Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phBYvO12NSCk"
   },
   "source": [
    "### [10 pts] Load the data + statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBn4tpu5NSCk"
   },
   "source": [
    "#### - Load the dataset: airbnb/AB_NYC_2019.csv and display the first 5 few rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962960,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "2M0c348nNSCk"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIBVnHCDNSCk"
   },
   "source": [
    "#### - Pull up info on the data type for each of the data fields. Will any of these be problematic feeding into your model (you may need to do a little research on this)? Discuss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962960,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "KrHJy8xbNSCl"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTLP-8wGNSCl"
   },
   "source": [
    "[Response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgKtipDHNSCl"
   },
   "source": [
    "#### - Drop the following columns: name, id, host_id, host_name, last_review, and reviews_per_month and display first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962960,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "7obKjXNrNSCl"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNKojHTDNSCl"
   },
   "source": [
    "#### - Display a summary of the statistics of the loaded data using .describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1680224962960,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "BVn_8LbyNSCl"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuZBH-0qNSCl"
   },
   "source": [
    "### [10 pts] Plot [boxplots](https://en.wikipedia.org/wiki/Box_plot) for the following 3 features: availability_365, number_of_reviews, price\n",
    "\n",
    "You may use either pandas or matplotlib to plot the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962961,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "QrmnFySWNSCl"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_Vx4bhENSCl"
   },
   "source": [
    "#### - What do you observe from the boxplot about the features? Anything suprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEI_J3t3NSCl"
   },
   "source": [
    "[Response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7S4AIRYNSCl"
   },
   "source": [
    "### [10 pts] Plot median price of a listing per neighbourhood_group using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962961,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "3zz_2QGzNSCl"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vyeTYarNSCl"
   },
   "source": [
    "#### - Describe what you expected to see with these features and what you actually observed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhQ9uqZxNSCm"
   },
   "source": [
    "[Response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhQLj8JENSCm"
   },
   "source": [
    "#### - So we can see different neighborhoods have dramatically different pricepoints, but how does the price breakdown by range. To see let's do a histogram of price by neighborhood to get a better sense of the distribution. \n",
    "\n",
    "To prevent outliers from affecting the histogram, use the input *range = [0,300]* in the histogram function which will upperbound the max price to 300 and ignore the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1680224962961,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "V2zLW__JNSCm"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wHocwv1NSCm"
   },
   "source": [
    "### [5 pts] Plot a map of airbnbs throughout New York. You do not need to overlay a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962962,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "FPZqEU5BNSCm"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFqAoWLNNSCm"
   },
   "source": [
    "### [10 pts] Plot median price of room types who have availability greater than 180 days and neighbourhood_group is Manhattan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962962,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "VtmajJCCNSCm"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZLbdml0NSCm"
   },
   "source": [
    "### [5 pts] Find features that correlate with price\n",
    "Using the correlation matrix:\n",
    "- which features have positive correlation with the price?\n",
    "- which features have negative correlation with the price?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962962,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "EobtceqnNSCm"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPRXzb2ENSCm"
   },
   "source": [
    "[Response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UghUiLrwNSCm"
   },
   "source": [
    "#### - Plot the full Scatter Matrix to see the correlation between prices and the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962962,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "bbzrhcYSNSCn"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzCY-RNkNSCn"
   },
   "source": [
    "## [30 pts] Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMBB0oijNSCn"
   },
   "source": [
    "### [5 pts] Partition the data into the features and the target data. The target data is price. Then partition the feature data into categorical and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "aborted",
     "timestamp": 1680224962963,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "eb3eEcMzNSCn"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeYvdG_5NSCn"
   },
   "source": [
    "### [10 pts] Create a scikit learn Transformer that augments the numerical data with the following two features \n",
    "\n",
    "- Max_yearly_bookings = availability_365 / minimum_nights\n",
    "\n",
    "- Distance from airbnb to the NYC JFK Airport \n",
    "    - Latitude: 40.641766 , Longitude: -73.780968\n",
    "\n",
    "Make sure to append these new features in this order.\n",
    "\n",
    "You may use the previously defined distance_func for the distance calculation.\n",
    "\n",
    "Note that this Transformer will be applied after imputation so we do not have to worry about Nulls in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962963,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "APgSY-qUNSCn"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amLWpBe-NSCn"
   },
   "source": [
    "#### -Test your new agumentation class by applying it to the numerical data you created. Print out the first 3 rows of the resultant data.\n",
    "\n",
    "Do not worry about missing data since none of the features we used involved nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962963,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "CSPcDDtPNSCn"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1ygtC0oNSCn"
   },
   "source": [
    "### [10 pts] Create a sklearn pipeline that performs the following operations of the feature data\n",
    "\n",
    "Now, we will create a full pipeline that processes the data before creating the model.\n",
    "\n",
    "For the numerical data, perfrom the following operations in order:\n",
    "- Use a SimpleImputer that imputes using the median value\n",
    "- Use the custom feature augmentation made in the previous part\n",
    "- Use StandardScaler to standardize the mean and standard deviation\n",
    "\n",
    "For categorical features, perform the following:\n",
    "- Perform one hot encoding on all the remaining categorical features: {neighbourhood_group, room_type} \n",
    "\n",
    "**After making the pipeline, perform the transform operation on the feature data and print out the first 3 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1680224962963,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "wv9bixghNSCn"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ummFaTIONSCn"
   },
   "source": [
    "### [5 pts] Set aside 20% of the data as test test (80% train, 20% test). Apply previously created pipeline to the train and test data separately as shown in the introduction example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "aborted",
     "timestamp": 1680224962964,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "hdE0-FySNSCn"
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXB5QLtONSCn"
   },
   "source": [
    "## [20 pts] Fit a Linear Regression Model\n",
    "\n",
    "The task is to predict the price, you could refer to the housing example on how to train and evaluate your model using the mean squared error (MSE).\n",
    "Provide both test and train set MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "aborted",
     "timestamp": 1680224962964,
     "user": {
      "displayName": "JAYANTH SHREEKUMAR",
      "userId": "00412995963591553851"
     },
     "user_tz": 420
    },
    "id": "IZFj8C5zNSCo"
   },
   "outputs": [],
   "source": [
    "#Your codes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
